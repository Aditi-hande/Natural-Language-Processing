{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40253cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score,recall_score,precision_score\n",
    "\n",
    "import contractions\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "\n",
    "import gensim.downloader as api\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, SimpleRNN, Embedding, GRU, LSTM,BatchNormalization\n",
    "\n",
    "#these are all the import statements for the dependencies\n",
    "#gensim verison is 4.3.0 and that needs to be used to run this code\n",
    "#DO NO RUN THIS ON COLAB THE CODE IS DIFFERENT DUE TO GENSIM VERSION\n",
    "# THERE IS A POSSIBILITY OF 2 NOT WORKING AS EACH TIME 60K REVIEWS MIGHT BE DIFFERENT\n",
    "# REST OF THE CODE SHOULD RUN JUST FINE IN ONE GO\n",
    "#SINCE I AM NOT ABLE TO RUN THIS ON MY PC DUE TO RAM ISSUES THE OUTPUTS DISPLAYED HERE ARE NOT THE VALID ONES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b61247ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset: https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Beauty_v1_00.tsv.gz\n",
    "data=pd.read_csv(\"amazon_reviews_us_Beauty_v1_00.tsv\",sep='\\t',on_bad_lines='skip')\n",
    "df=data.loc[:,[\"review_body\",\"star_rating\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50ca8f0",
   "metadata": {},
   "source": [
    "# Dataset Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc4c793b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classfunc(star_): # defines func to cluster reviwes based on rating   \n",
    "    if star_=='5'or star_=='4':\n",
    "        return 3\n",
    "    elif star_=='3':\n",
    "        return 2\n",
    "    elif star_ =='2'or star_ =='1':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "df['star_rating'] = df['star_rating'].astype(str) # converting the column's entries to string\n",
    "df['class'] = df['star_rating'].apply(lambda x: classfunc(x[0])) # applying the clustering custom function on all rows\n",
    "df.drop(df[(df['class'] == 0)].index, inplace=True) #dropping all entries with incorrect/invalid data\n",
    "df = df.dropna(subset=['review_body']) #removing all entries with NA\n",
    "#df = df[df['review_body'].apply(lambda x: len(x.split())>= 15)]\n",
    "\n",
    "\n",
    "\n",
    "df_balanced = pd.DataFrame()\n",
    "df_balanced = df.groupby([\"class\"]).apply(lambda grp: grp.sample(n=20000)) #selecting 2k entries from all three classes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcb9bc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balanced['review_body'] = df_balanced['review_body'].str.lower() #converting all reviews to lower case\n",
    "df_balanced['review_body'] = df_balanced['review_body'].str.replace('http\\S+|www.\\S+', '', case=False) # removing all urls\n",
    "df_balanced['review_body'] = df_balanced['review_body'].str.replace('[^a-zA-Z ]', '') #removing non alphabetical entries\n",
    "X=df_balanced.review_body\n",
    "# removing html tags from review text\n",
    "df_balanced['review_body'] = [BeautifulSoup(X).get_text() for X in df_balanced['review_body'].astype(str) ]\n",
    "df_balanced['review_body'] = df_balanced['review_body'].str.strip()\n",
    "\n",
    "df_balanced['review_body'] = df_balanced['review_body'].apply(lambda x: [contractions.fix(word) for word in x.split()]) #applying contractions\n",
    "#df_balanced['review_body'] = [' '.join(map(str, word)) for word in df_balanced['review_body']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6395fa46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_balanced['review_body'] = [' '.join(map(str, word)) for word in df_balanced['review_body']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df4e0497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the DataFrame to a CSV file with the header and without the index\n",
    "#df_balanced.to_csv('balanceddata.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb585c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_balanced1=pd.read_csv(\"balanceddata.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc657d89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_body</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>it just like you spray the white powder on you...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>it was a mixture of liquid soap and something ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>it did not do anything for me except make my c...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am sitting here with a half shaven beard as ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i have tried many products most do a little to...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         review_body  star_rating  class\n",
       "0  it just like you spray the white powder on you...          1.0      1\n",
       "1  it was a mixture of liquid soap and something ...          1.0      1\n",
       "2  it did not do anything for me except make my c...          1.0      1\n",
       "3  i am sitting here with a half shaven beard as ...          1.0      1\n",
       "4  i have tried many products most do a little to...          1.0      1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_balanced1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2f456b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from nltk.corpus import stopwords\n",
    "\n",
    "#stop words removal\n",
    "#english_stopwords = set(stopwords.words('english')) - set(['not', 'no'])\n",
    "# print(english_stopwords)\n",
    "\n",
    "#df_balanced['review_body']= df_balanced['review_body'].apply(lambda x: [item for item in x.split() if item not in english_stopwords])\n",
    "#df_balanced['review_body'] = [' '.join(map(str, word)) for word in df_balanced['review_body']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aee7fae",
   "metadata": {},
   "source": [
    "# Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01dfb34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = api.load('word2vec-google-news-300')\n",
    "# using gensim's wv and loading it using downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1255c4f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f3b236",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.most_similar(positive=[\"water\", \"thick\"], negative=[\"thin\"])\n",
    "\n",
    "result1 = model.most_similar(\"amazing\")\n",
    "result2 = model.similarity(\"good\",\"money\")\n",
    "result3 = model.similarity(\"good\",\"great\")\n",
    "\n",
    "\n",
    "print(\"water+thick-thin = \",result[0][0])# this gives multiple words ans its score so printing highest\n",
    "print(\"most similar word to amazing = \",result1[0][0], \"with similarity score of \", result1[0][1])\n",
    "print(\"similarity between good and money and good and great \",result2,result3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bc0e44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import gensim\n",
    "cores = multiprocessing.cpu_count() # finds no of core so that we can use i to mention workers in mymodel\n",
    "\n",
    "#defining my own model as hyperparams mentioned in pdf\n",
    "mymodel = gensim.models.Word2Vec(\n",
    "            window = 13,\n",
    "            vector_size = 300,\n",
    "            min_count = 9,\n",
    "            workers = cores-1\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "786b423d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10398497, 14985240)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#building vocab using predefined method\n",
    "#training model on my reviews\n",
    "\n",
    "mymodel.build_vocab(df_balanced.review_body, progress_per = 10000)\n",
    "mymodel.train(df_balanced.review_body, total_examples = mymodel.corpus_count, epochs = mymodel.epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "daa0d12b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>review_body</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">3</th>\n",
       "      <th>4965110</th>\n",
       "      <td>[lightweight, rubberized, which, makes, it, ea...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2352657</th>\n",
       "      <td>[it, does not, take, up, much, room, on, the, ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">2</th>\n",
       "      <th>3961515</th>\n",
       "      <td>[i have, used, other, products, in, the, past,...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3671506</th>\n",
       "      <td>[was, thicker, than, i, expected, but, i, thin...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3018456</th>\n",
       "      <td>[i, have, been, using, this, for, about, two, ...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <th>3809434</th>\n",
       "      <td>[this, is, my, favorite, perfume, for, my, boy...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">2</th>\n",
       "      <th>2440435</th>\n",
       "      <td>[my, wife, got, stretch, mark, and, used, it, ...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1482067</th>\n",
       "      <td>[this, one, disappointed, me, with, it, uncomf...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>1474646</th>\n",
       "      <td>[it, shipped, pretty, fast, however, it, says,...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <th>4915382</th>\n",
       "      <td>[i, was, not, too, crazy, about, this, it, sti...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>1203675</th>\n",
       "      <td>[fake, bake, is, the, way, to, go, you, can, b...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <th>2858872</th>\n",
       "      <td>[its, a, very, cute, purse, not, what, i, expe...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <th>1356431</th>\n",
       "      <td>[very, good]</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>4940376</th>\n",
       "      <td>[i, was, very, disappointed, with, this, produ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4560545</th>\n",
       "      <td>[i, purchased, this, hair, from, my, local, bs...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <th>1320184</th>\n",
       "      <td>[great, product, really, does, its, job]</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <th>3056282</th>\n",
       "      <td>[i, have, seen, rave, reviews, of, this, shade...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>4023929</th>\n",
       "      <td>[it, really, did not, fit, my, skin, although,...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <th>2045484</th>\n",
       "      <td>[made, my, skin, even, more, oily]</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <th>3331465</th>\n",
       "      <td>[fresh, clean, scent, my, absolute, favorite, ...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>524855</th>\n",
       "      <td>[this, product, does, not, hold, my, hair, at,...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <th>2698763</th>\n",
       "      <td>[this, kit, came, in, handy, for, me, to, do, ...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>4404898</th>\n",
       "      <td>[i, have, used, this, product, exactly, as, su...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">3</th>\n",
       "      <th>378889</th>\n",
       "      <td>[love, this, sunscreenmy, dermatologist, recom...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1625896</th>\n",
       "      <td>[seems, to, working, well]</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>2371644</th>\n",
       "      <td>[bought, it, for, a, friend, it, was not, as, ...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">2</th>\n",
       "      <th>1959772</th>\n",
       "      <td>[the, one, i, got, was, not, white, at, all, i...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2463573</th>\n",
       "      <td>[works, fine, but, the, scent, is, off, puttin...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2616583</th>\n",
       "      <td>[it, worked, great, on, your, hair, but, stopp...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <th>1262422</th>\n",
       "      <td>[mamas, favorite]</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">2</th>\n",
       "      <th>4345794</th>\n",
       "      <td>[if, you, saw, videos, on, youtube, about, thi...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3825864</th>\n",
       "      <td>[pros, love, having, fun, hairbr, cons, gets, ...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1042337</th>\n",
       "      <td>[i, cannot, comment, on, the, purity, of, this...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>3370612</th>\n",
       "      <td>[this, scraper, is, so, flimsy, that, it, does...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2511825</th>\n",
       "      <td>[dried, out, and, irritated, my, skin, for, ab...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">3</th>\n",
       "      <th>3009666</th>\n",
       "      <td>[this, is, a, great, product, and, works, well...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2088176</th>\n",
       "      <td>[excellent]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1804461</th>\n",
       "      <td>[awesome, thank, you]</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238081</th>\n",
       "      <td>[good, cream, but, expensive]</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <th>919825</th>\n",
       "      <td>[was, a, gift, and, she, loved, it]</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>4560892</th>\n",
       "      <td>[i, swear, by, this, stuff, if, you, use, it, ...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <th>3002900</th>\n",
       "      <td>[my, almost, year, old, has, sucked, his, fore...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <th>4122756</th>\n",
       "      <td>[it, took, me, several, tries, to, finally, ge...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>4159987</th>\n",
       "      <td>[i, used, this, a, few, times, now, as, a, top...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433593</th>\n",
       "      <td>[these, are, the, worst, clippers, on, the, pl...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <th>1431315</th>\n",
       "      <td>[this, looks, like, a, barbie, comb, its, real...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <th>3263247</th>\n",
       "      <td>[this, creme, worked, very, well, it, helped, ...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">2</th>\n",
       "      <th>3437952</th>\n",
       "      <td>[a, little, pricy, but, sometimes, you, just, ...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4680704</th>\n",
       "      <td>[nice, bubbles, lavender, water, but, not, muc...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <th>4285369</th>\n",
       "      <td>[i, bought, this, for, my, husband, he, has, b...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     review_body star_rating  \\\n",
       "class                                                                          \n",
       "3     4965110  [lightweight, rubberized, which, makes, it, ea...           5   \n",
       "      2352657  [it, does not, take, up, much, room, on, the, ...         5.0   \n",
       "2     3961515  [i have, used, other, products, in, the, past,...           3   \n",
       "      3671506  [was, thicker, than, i, expected, but, i, thin...           3   \n",
       "      3018456  [i, have, been, using, this, for, about, two, ...           3   \n",
       "3     3809434  [this, is, my, favorite, perfume, for, my, boy...           5   \n",
       "2     2440435  [my, wife, got, stretch, mark, and, used, it, ...           3   \n",
       "      1482067  [this, one, disappointed, me, with, it, uncomf...           3   \n",
       "1     1474646  [it, shipped, pretty, fast, however, it, says,...           1   \n",
       "2     4915382  [i, was, not, too, crazy, about, this, it, sti...           3   \n",
       "1     1203675  [fake, bake, is, the, way, to, go, you, can, b...           1   \n",
       "2     2858872  [its, a, very, cute, purse, not, what, i, expe...           3   \n",
       "3     1356431                                       [very, good]           5   \n",
       "1     4940376  [i, was, very, disappointed, with, this, produ...           1   \n",
       "      4560545  [i, purchased, this, hair, from, my, local, bs...           1   \n",
       "3     1320184           [great, product, really, does, its, job]           5   \n",
       "2     3056282  [i, have, seen, rave, reviews, of, this, shade...           3   \n",
       "1     4023929  [it, really, did not, fit, my, skin, although,...           1   \n",
       "2     2045484                 [made, my, skin, even, more, oily]         3.0   \n",
       "3     3331465  [fresh, clean, scent, my, absolute, favorite, ...           5   \n",
       "1     524855   [this, product, does, not, hold, my, hair, at,...           1   \n",
       "2     2698763  [this, kit, came, in, handy, for, me, to, do, ...           3   \n",
       "1     4404898  [i, have, used, this, product, exactly, as, su...           1   \n",
       "3     378889   [love, this, sunscreenmy, dermatologist, recom...           4   \n",
       "      1625896                         [seems, to, working, well]           4   \n",
       "1     2371644  [bought, it, for, a, friend, it, was not, as, ...           2   \n",
       "2     1959772  [the, one, i, got, was, not, white, at, all, i...           3   \n",
       "      2463573  [works, fine, but, the, scent, is, off, puttin...           3   \n",
       "      2616583  [it, worked, great, on, your, hair, but, stopp...           3   \n",
       "3     1262422                                  [mamas, favorite]           5   \n",
       "2     4345794  [if, you, saw, videos, on, youtube, about, thi...           3   \n",
       "      3825864  [pros, love, having, fun, hairbr, cons, gets, ...           3   \n",
       "      1042337  [i, cannot, comment, on, the, purity, of, this...           3   \n",
       "1     3370612  [this, scraper, is, so, flimsy, that, it, does...           1   \n",
       "      2511825  [dried, out, and, irritated, my, skin, for, ab...           1   \n",
       "3     3009666  [this, is, a, great, product, and, works, well...           5   \n",
       "      2088176                                        [excellent]         5.0   \n",
       "      1804461                              [awesome, thank, you]           5   \n",
       "      238081                       [good, cream, but, expensive]           4   \n",
       "2     919825                 [was, a, gift, and, she, loved, it]           3   \n",
       "1     4560892  [i, swear, by, this, stuff, if, you, use, it, ...           2   \n",
       "3     3002900  [my, almost, year, old, has, sucked, his, fore...           5   \n",
       "2     4122756  [it, took, me, several, tries, to, finally, ge...           3   \n",
       "1     4159987  [i, used, this, a, few, times, now, as, a, top...           2   \n",
       "      433593   [these, are, the, worst, clippers, on, the, pl...           1   \n",
       "2     1431315  [this, looks, like, a, barbie, comb, its, real...         3.0   \n",
       "3     3263247  [this, creme, worked, very, well, it, helped, ...           4   \n",
       "2     3437952  [a, little, pricy, but, sometimes, you, just, ...           3   \n",
       "      4680704  [nice, bubbles, lavender, water, but, not, muc...           3   \n",
       "3     4285369  [i, bought, this, for, my, husband, he, has, b...           5   \n",
       "\n",
       "               class  \n",
       "class                 \n",
       "3     4965110      3  \n",
       "      2352657      3  \n",
       "2     3961515      2  \n",
       "      3671506      2  \n",
       "      3018456      2  \n",
       "3     3809434      3  \n",
       "2     2440435      2  \n",
       "      1482067      2  \n",
       "1     1474646      1  \n",
       "2     4915382      2  \n",
       "1     1203675      1  \n",
       "2     2858872      2  \n",
       "3     1356431      3  \n",
       "1     4940376      1  \n",
       "      4560545      1  \n",
       "3     1320184      3  \n",
       "2     3056282      2  \n",
       "1     4023929      1  \n",
       "2     2045484      2  \n",
       "3     3331465      3  \n",
       "1     524855       1  \n",
       "2     2698763      2  \n",
       "1     4404898      1  \n",
       "3     378889       3  \n",
       "      1625896      3  \n",
       "1     2371644      1  \n",
       "2     1959772      2  \n",
       "      2463573      2  \n",
       "      2616583      2  \n",
       "3     1262422      3  \n",
       "2     4345794      2  \n",
       "      3825864      2  \n",
       "      1042337      2  \n",
       "1     3370612      1  \n",
       "      2511825      1  \n",
       "3     3009666      3  \n",
       "      2088176      3  \n",
       "      1804461      3  \n",
       "      238081       3  \n",
       "2     919825       2  \n",
       "1     4560892      1  \n",
       "3     3002900      3  \n",
       "2     4122756      2  \n",
       "1     4159987      1  \n",
       "      433593       1  \n",
       "2     1431315      2  \n",
       "3     3263247      3  \n",
       "2     3437952      2  \n",
       "      4680704      2  \n",
       "3     4285369      3  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## we can conclude that pretrained model has better vocab so its vectors capture more dependencies in them\n",
    "## we can also see that similarity values ans the most similar words in case of pretrained make more sense as the vectors are better represented.\n",
    "## so we say tht pretrained vectors is better "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa2417bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "King − M an + W oman =  rinse\n",
      "most similar word to amazing =  awesome with similarity score of  0.8327875733375549\n",
      "similarity between good and money and good and great  -0.015617784 0.7993416\n",
      "similarity between thick and thin 0.7993416\n"
     ]
    }
   ],
   "source": [
    "\n",
    "result = mymodel.wv.most_similar(positive=[\"water\", \"thick\"], negative=[\"thin\"])\n",
    "result1 = mymodel.wv.most_similar(\"amazing\")\n",
    "result2 = mymodel.wv.similarity(\"good\",\"money\")\n",
    "result3 = mymodel.wv.similarity(\"good\",\"great\")\n",
    "result3 = mymodel.wv.similarity(\"thick\",\"thin\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"water − thin + thick = \",result[0][0])\n",
    "print(\"most similar word to amazing = \",result1[0][0], \"with similarity score of \", result1[0][1])\n",
    "print(\"similarity between good and money and good and great \",result2,result3)\n",
    "print(\"similarity between thick and thin\", result3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77780008",
   "metadata": {},
   "source": [
    "# Simple models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "18adf914",
   "metadata": {},
   "outputs": [],
   "source": [
    "##In this cell, for each entry in review, first i am checing if that word is present in the vocabulary is yes im adding its vector to vectors\n",
    "## and later making sure that there are valid entries if not appending vectors as entries zeros, and if yes then appending mean of the vectors.\n",
    "\n",
    "vectorized_x = []\n",
    "for review in df_balanced['review_body']:\n",
    "    vectors = []\n",
    "    for word in review:\n",
    "        if word in model.key_to_index:\n",
    "            vectors.append(model.get_vector(word))\n",
    "    if len(vectors) > 0:\n",
    "        vectorized_x.append(np.mean(vectors, axis=0))\n",
    "    else:\n",
    "        vectorized_x.append(np.zeros(300))\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "034b263e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=df_balanced['class']\n",
    "  \n",
    "# using the train test split function with stratify so that we get balanced data\n",
    "X_train, X_test,y_train, y_test = train_test_split(vectorized_x,y ,\n",
    "                                   random_state=46, \n",
    "                                   test_size=0.20,\n",
    "                                    stratify = y,\n",
    "                                   shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc47918",
   "metadata": {},
   "source": [
    "# Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "80203a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision, recall, f1score for class 1:  0.4356101674497049 ,  0.904 ,  0.5879196813267215\n",
      "precision, recall, f1score for class 2:  0.5544440169295883 ,  0.36025 ,  0.43673283830883475\n",
      "precision, recall, f1score for class 3:  0.9154545454545454 ,  0.25175 ,  0.39490196078431367\n",
      "precision, recall, f1score   average  :  0.6351695766112795 ,  0.5053333333333333 ,  0.47318482680662327\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import multiclass\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "# Used sklearn module for inbuilt model of perceptron with random state as 50\n",
    "# (could be any feasible value). This was done using\n",
    "# Perceptron(random_state=50)\n",
    "#  Fit the model on training data and later predicted the y_hat using the .predict()\n",
    "# function.\n",
    "#  Used the actual y values and yhat (predicted) values to calculate recall precision\n",
    "# and f1 scores using the sklearn.metrics.\n",
    "# Extracted the per class values from the generated metrics printed those and for\n",
    "# the average calculated the metric again with parameter specifying that average\n",
    "# should be weighted.\n",
    "\n",
    "\n",
    "model_perceptron = Perceptron(random_state=50)\n",
    "model_perceptron.fit(X_train, y_train)\n",
    "\n",
    "y_hat = model_perceptron.predict(X_test)\n",
    "\n",
    "precision_perceptron = precision_score(y_test,y_hat,average=None)\n",
    "recall_perceptron = recall_score(y_test,y_hat,average=None)\n",
    "f1_perceptron = f1_score(y_test,y_hat,average=None)\n",
    "\n",
    "print(\"For the current assignment with word2vec features\")\n",
    "\n",
    "print(\"precision, recall, f1score for class 1: \", precision_perceptron[0], \", \", recall_perceptron[0], \", \", f1_perceptron[0])\n",
    "print(\"precision, recall, f1score for class 2: \", precision_perceptron[1], \", \", recall_perceptron[1], \", \", f1_perceptron[1])\n",
    "print(\"precision, recall, f1score for class 3: \", precision_perceptron[2], \", \", recall_perceptron[2], \", \", f1_perceptron[2])\n",
    "print(\"precision, recall, f1score   average  : \", precision_score(y_test,y_hat,average='weighted'), \", \", recall_score(y_test,y_hat,average='weighted'), \", \", f1_score(y_test,y_hat,average='weighted'))\n",
    "\n",
    "print(\"for HW1 where we used tfidf features (avg of all classes's) precision was 0.651, recall was 0.64, f1 score was 0.65 \")\n",
    "\n",
    "\n",
    "#print(classification_report(model_perceptron.predict(X_test), y_test))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf06479",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aa27ca05",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f5fa15ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision, recall, f1score for class 1:  0.6559314179796107 ,  0.70775 ,  0.6808561808561809\n",
      "precision, recall, f1score for class 2:  0.6003143006809848 ,  0.573 ,  0.5863392171910975\n",
      "precision, recall, f1score for class 3:  0.7382307294361097 ,  0.7135 ,  0.7256547165013985\n",
      "precision, recall, f1score   average  :  0.6648254826989017 ,  0.66475 ,  0.6642833715162257\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "\n",
    "# Used sklearn module for inbuilt model of SVM with random state as 4 (could be\n",
    "# any feasible value). This was done using LinearSVC(random_state=4)\n",
    "# ● Fit the model on training data and later predicted the y_hat using the .predict()\n",
    "# function.\n",
    "# ● Used the actual y values and yhat (predicted) values to calculate recall precision\n",
    "# and f1 scores using the sklearn.metrics.\n",
    "# ● Extracted the per class values from the generated metrics printed those and for\n",
    "# the average calculated the metric again with parameter specifying that the\n",
    "# average should be weighted.\n",
    "\n",
    "model_svc = LinearSVC(random_state=4)\n",
    "model_svc.fit(X_train, y_train)\n",
    "\n",
    "y_hat = model_svc.predict(X_test)\n",
    "\n",
    "precision_svc = precision_score(y_test,y_hat,average=None)\n",
    "recall_svc = recall_score(y_test,y_hat,average=None)\n",
    "f1_svc = f1_score(y_test,y_hat,average=None)\n",
    "\n",
    "print(\"For the current assignment with word2vec features\")\n",
    "print(\"precision, recall, f1score for class 1: \", precision_svc[0], \", \", recall_svc[0], \", \", f1_svc[0])\n",
    "print(\"precision, recall, f1score for class 2: \", precision_svc[1], \", \", recall_svc[1], \", \", f1_svc[1])\n",
    "print(\"precision, recall, f1score for class 3: \", precision_svc[2], \", \", recall_svc[2], \", \", f1_svc[2])\n",
    "print(\"precision, recall, f1score   average  : \", precision_score(y_test,y_hat,average='weighted'), \", \", recall_score(y_test,y_hat,average='weighted'), \", \", f1_score(y_test,y_hat,average='weighted'))\n",
    "\n",
    "print(\"for HW1 where we used tfidf features (avg of all classes's) precision was 0.687, recall was 0.689, f1 score was 0.688 \")\n",
    "\n",
    "#We can conclude that tfidf features give better results as they use local data to find features and that quality has helped perceptron and SVM\n",
    "# so we can conclude that SVM and perceptron work better when features represnt local data as in case of tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29423f2",
   "metadata": {},
   "source": [
    "# Feedforward Neural Networks (4a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "affdd6d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1500/1500 [==============================] - 7s 4ms/step - loss: 0.8430 - accuracy: 0.6143 - val_loss: 0.7824 - val_accuracy: 0.6536\n",
      "Epoch 2/10\n",
      "1500/1500 [==============================] - 7s 5ms/step - loss: 0.7804 - accuracy: 0.6523 - val_loss: 0.8143 - val_accuracy: 0.6261\n",
      "Epoch 3/10\n",
      "1500/1500 [==============================] - 8s 5ms/step - loss: 0.7624 - accuracy: 0.6607 - val_loss: 0.7964 - val_accuracy: 0.6424\n",
      "Epoch 4/10\n",
      "1500/1500 [==============================] - 7s 5ms/step - loss: 0.7477 - accuracy: 0.6684 - val_loss: 0.7519 - val_accuracy: 0.6678\n",
      "Epoch 5/10\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.7391 - accuracy: 0.6734 - val_loss: 0.7410 - val_accuracy: 0.6747\n",
      "Epoch 6/10\n",
      "1500/1500 [==============================] - 8s 5ms/step - loss: 0.7300 - accuracy: 0.6774 - val_loss: 0.7471 - val_accuracy: 0.6695\n",
      "Epoch 7/10\n",
      "1500/1500 [==============================] - 9s 6ms/step - loss: 0.7218 - accuracy: 0.6826 - val_loss: 0.7361 - val_accuracy: 0.6765\n",
      "Epoch 8/10\n",
      "1500/1500 [==============================] - 5s 4ms/step - loss: 0.7157 - accuracy: 0.6850 - val_loss: 0.7335 - val_accuracy: 0.6749\n",
      "Epoch 9/10\n",
      "1500/1500 [==============================] - 8s 5ms/step - loss: 0.7085 - accuracy: 0.6875 - val_loss: 0.7311 - val_accuracy: 0.6816\n",
      "Epoch 10/10\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.7014 - accuracy: 0.6912 - val_loss: 0.7493 - val_accuracy: 0.6671\n",
      "375/375 [==============================] - 2s 3ms/step - loss: 0.7311 - accuracy: 0.6816\n",
      "Test accuracy of fnn: 68.15833449363708\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define the multilayer perceptron network with activations as relu for first 2 layers and then set the final as softmax\n",
    "#using relu so that model is non linear and learns complex relations\n",
    "#\n",
    "model_fnn = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(100, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='relu'),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "#used categorical_crossentropy for faster convergence and high penalty for mistakes\n",
    "#used adams for adaptive learning rate, fater concergence and as it requires less memory\n",
    "model_fnn.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "X_trainfnn = np.array(X_train)\n",
    "X_testfnn = np.array(X_test)\n",
    "y_trainfnn = np.array(y_train)\n",
    "y_testfnn = np.array(y_test)\n",
    "\n",
    "y_trainfnn = to_categorical(y_trainfnn-1,num_classes = 3)\n",
    "y_testfnn = to_categorical(y_testfnn-1,num_classes = 3)\n",
    "\n",
    "#creating the checkpoint to save the best epocs model\n",
    "checkpoint = ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True)\n",
    "\n",
    "#fitting the model with hyperparameters that gave decent scores\n",
    "history = model_fnn.fit(X_trainfnn, y_trainfnn, epochs=10, batch_size=32, shuffle=True, validation_data=(X_testfnn, y_testfnn),callbacks=[checkpoint])\n",
    "best_model = load_model('best_model.h5')\n",
    "\n",
    "# Evaluate the performance of the trained model\n",
    "test_loss, test_acc = best_model.evaluate(X_testfnn, y_testfnn)\n",
    "\n",
    "print('Test accuracy of fnn:', test_acc*100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159b5f25",
   "metadata": {},
   "source": [
    "# Feedforward Neural Networks (4b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5baa4128",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_x = []\n",
    "\n",
    "#since we need vectors to be concatenated vectors of first 10 words in each review, writing this code to achieve that\n",
    "\n",
    "\n",
    "def vectorizex(review):\n",
    "#for review in df_balanced['review_body']:\n",
    "    vectors = []\n",
    "    for word in review:\n",
    "        if word in model.key_to_index:\n",
    "            vectors.append(model.get_vector(word))\n",
    "    if len(vectors)<10:\n",
    "        while(len(vectors)!=10):\n",
    "            vectors.append(np.zeros(300))\n",
    "    #print(len(vectors))        \n",
    "    return np.concatenate(vectors[:10])\n",
    "\n",
    "for review in df_balanced['review_body']:\n",
    "    vec_x.append(vectorizex(review))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "28cb951f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y=df_balanced['class']\n",
    "  \n",
    "# using the train test split function\n",
    "X_train_1, X_test_1,y_train_1, y_test_1 = train_test_split(vec_x,y ,\n",
    "                                   random_state=46, \n",
    "                                   test_size=0.20,\n",
    "                                    stratify = y,\n",
    "                                   shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "127609d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1500/1500 [==============================] - 24s 11ms/step - loss: 0.9182 - accuracy: 0.5494 - val_loss: 0.8879 - val_accuracy: 0.5715\n",
      "Epoch 2/10\n",
      "1500/1500 [==============================] - 15s 10ms/step - loss: 0.8217 - accuracy: 0.6139 - val_loss: 0.8869 - val_accuracy: 0.5768\n",
      "Epoch 3/10\n",
      "1500/1500 [==============================] - 15s 10ms/step - loss: 0.7422 - accuracy: 0.6633 - val_loss: 0.9136 - val_accuracy: 0.5756\n",
      "Epoch 4/10\n",
      "1500/1500 [==============================] - 14s 10ms/step - loss: 0.6398 - accuracy: 0.7187 - val_loss: 0.9905 - val_accuracy: 0.5623\n",
      "Epoch 5/10\n",
      "1500/1500 [==============================] - 14s 9ms/step - loss: 0.5257 - accuracy: 0.7762 - val_loss: 1.1189 - val_accuracy: 0.5549\n",
      "Epoch 6/10\n",
      "1500/1500 [==============================] - 13s 9ms/step - loss: 0.4216 - accuracy: 0.8271 - val_loss: 1.3032 - val_accuracy: 0.5452\n",
      "Epoch 7/10\n",
      "1500/1500 [==============================] - 13s 9ms/step - loss: 0.3321 - accuracy: 0.8675 - val_loss: 1.4645 - val_accuracy: 0.5446\n",
      "Epoch 8/10\n",
      "1500/1500 [==============================] - 13s 9ms/step - loss: 0.2660 - accuracy: 0.8961 - val_loss: 1.7260 - val_accuracy: 0.5378\n",
      "Epoch 9/10\n",
      "1500/1500 [==============================] - 13s 9ms/step - loss: 0.2187 - accuracy: 0.9149 - val_loss: 1.9333 - val_accuracy: 0.5343\n",
      "Epoch 10/10\n",
      "1500/1500 [==============================] - 13s 9ms/step - loss: 0.1828 - accuracy: 0.9302 - val_loss: 2.2429 - val_accuracy: 0.5387\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.8869 - accuracy: 0.5768\n",
      "Test accuracy of fnn(4b): 57.68333077430725\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#using relu so that model is non linear and learns complex relations\n",
    "#used categorical_crossentropy for faster convergence and high penalty for mistakes\n",
    "#used adams for adaptive learning rate, faster concergence and as it requires less memory\n",
    "\n",
    "# Define the multilayer perceptron network\n",
    "model_fnn_4b = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(100, activation='relu',input_dim=3000),\n",
    "    tf.keras.layers.Dense(10, activation='relu'),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_fnn_4b.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "X_trainfnn_1 = np.array(X_train_1)\n",
    "X_testfnn_1 = np.array(X_test_1)\n",
    "y_trainfnn_1 = np.array(y_train_1)\n",
    "y_testfnn_1 = np.array(y_test_1)\n",
    "\n",
    "y_trainfnn_1 = to_categorical(y_trainfnn_1-1,num_classes = 3)\n",
    "y_testfnn_1 = to_categorical(y_testfnn_1-1,num_classes = 3)\n",
    "\n",
    "# X_trainfnn_1 = X_trainfnn_1.reshape(48000,3000)\n",
    "# X_testfnn_1 = X_testfnn_1.reshape(12000,3000)\n",
    "\n",
    "\n",
    "checkpoint = ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True)\n",
    "\n",
    "\n",
    "history = model_fnn_4b.fit(X_trainfnn_1, y_trainfnn_1, epochs=10, batch_size=32, shuffle=True, validation_data=(X_testfnn_1, y_testfnn_1),callbacks=[checkpoint])\n",
    "best_model = load_model('best_model.h5')\n",
    "\n",
    "# Evaluate the performance of the trained model\n",
    "#test_loss, test_acc = model_fnn.evaluate(X_testfnn, y_testfnn)\n",
    "test_loss, test_acc = best_model.evaluate(X_testfnn_1, y_testfnn_1)\n",
    "\n",
    "print('Test accuracy of fnn(4b):', test_acc*100)\n",
    "\n",
    "##What do you conclude by comparing accuracy values you obtain with\n",
    "##those obtained in the “’Simple Models” section.\n",
    "\n",
    "##Accuracy for both cases of fnn is better as fnn perform better on non linear relationships which is the case in word2vec\n",
    "##fnn seems to capture distributed relationships better \n",
    "##FNNs are more flexible than SVM or Perceptron\n",
    "##word2vec has more dimentionality as it represents more data which is used better by fnn than simple models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2057fb2e",
   "metadata": {},
   "source": [
    "# RNN (5a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b8edaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_x = []\n",
    "\n",
    "## since we wanted to train rnn insted of using feature vectors we use the vectors representing the indices of the word in vocab\n",
    "## also since we want the length to be 20 we are padding if length is less. \n",
    "\n",
    "def idxx(review):\n",
    "    vectors = []\n",
    "    for word in review:\n",
    "        if word in model.key_to_index:\n",
    "            vectors.append( model.key_to_index[word])\n",
    "    if len(vectors)<20:\n",
    "        while(len(vectors)!=20):\n",
    "            vectors.append(0)\n",
    "           \n",
    "    return vectors[:20]\n",
    "\n",
    "for review in df_balanced['review_body']:\n",
    "    idx_x.append(idxx(review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cecb478",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y=df_balanced['class']\n",
    "  \n",
    "# using the train test split function\n",
    "X_train_2, X_test_2,y_train_2, y_test_2 = train_test_split(idx_x,y ,\n",
    "                                   random_state=42, \n",
    "                                   test_size=0.20,\n",
    "                                    stratify = y,\n",
    "                                   shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d3db36d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "The vocab attribute was removed from KeyedVector in Gensim 4.0.0.\nUse KeyedVector's .key_to_index dict, .index_to_key list, and methods .get_vecattr(key, attr) and .set_vecattr(key, attr, new_val) instead.\nSee https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m model_rnn \u001b[38;5;241m=\u001b[39m Sequential()\n\u001b[1;32m----> 2\u001b[0m model_rnn\u001b[38;5;241m.\u001b[39madd(Embedding(input_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab\u001b[49m), output_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m, weights\u001b[38;5;241m=\u001b[39m[model\u001b[38;5;241m.\u001b[39mvectors], input_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, trainable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[0;32m      3\u001b[0m model_rnn\u001b[38;5;241m.\u001b[39madd(SimpleRNN(\u001b[38;5;241m20\u001b[39m))\n\u001b[0;32m      4\u001b[0m model_rnn\u001b[38;5;241m.\u001b[39madd(Dense(\u001b[38;5;241m3\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gensim\\models\\keyedvectors.py:734\u001b[0m, in \u001b[0;36mKeyedVectors.vocab\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    732\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    733\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvocab\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 734\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m    735\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe vocab attribute was removed from KeyedVector in Gensim 4.0.0.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    736\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse KeyedVector\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms .key_to_index dict, .index_to_key list, and methods \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    737\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.get_vecattr(key, attr) and .set_vecattr(key, attr, new_val) instead.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    738\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSee https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    739\u001b[0m     )\n",
      "\u001b[1;31mAttributeError\u001b[0m: The vocab attribute was removed from KeyedVector in Gensim 4.0.0.\nUse KeyedVector's .key_to_index dict, .index_to_key list, and methods .get_vecattr(key, attr) and .set_vecattr(key, attr, new_val) instead.\nSee https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4"
     ]
    }
   ],
   "source": [
    "\n",
    "#using relu so that model is non linear and learns complex relations\n",
    "#used categorical_crossentropy for faster convergence and high penalty for mistakes\n",
    "#used adams for adaptive learning rate, faster concergence and as it requires less memory\n",
    "\n",
    "model_rnn = Sequential()\n",
    "model_rnn.add(Embedding(input_dim=len(model.key_to_index), output_dim=300, weights=[model.vectors], input_length=20, trainable=False))\n",
    "model_rnn.add(SimpleRNN(20))\n",
    "model_rnn.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model_rnn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "checkpoint = ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True)\n",
    "\n",
    "X_trainrnn_2 = np.array(X_train_2)\n",
    "X_testrnn_2 = np.array(X_test_2)\n",
    "y_trainrnn_2 = np.array(y_train_2)\n",
    "y_testrnn_2 = np.array(y_test_2)\n",
    "\n",
    "y_trainrnn_2 = to_categorical(y_trainrnn_2-1,num_classes = 3)\n",
    "y_testrnn_2 = to_categorical(y_testrnn_2-1,num_classes = 3)\n",
    "\n",
    "\n",
    "# Fit the model\n",
    "model_rnn.fit(X_trainrnn_2, y_trainrnn_2, batch_size=32, epochs=15, validation_data=(X_testrnn_2, y_testrnn_2),callbacks=[checkpoint])\n",
    "best_model = load_model('best_model.h5')\n",
    "\n",
    "\n",
    "# Evaluate the model on the testing set\n",
    "loss, accuracy = best_model.evaluate(X_testrnn_2, y_testrnn_2)\n",
    "print('Test loss:', loss)\n",
    "print('Test accuracy:', accuracy)\n",
    "\n",
    "\n",
    "\n",
    "# What do you conclude by comparing accuracy values you obtain with\n",
    "# those obtained with feedforward neural network models.\n",
    "\n",
    "##RNN is performing better in this case as our data is sort of sequential in nature so rnns are known to perform better on them\n",
    "#fnns are not as efficient in capturing long term dependencies than rnn so that might be one of the reasons by rnn has better accuracy\n",
    "## rnns are maybe performing better as context of word is better captured in even limited vocabulary due to their memory\n",
    "##also due to large size of data and rnns being more complax that could be one of the reasons\n",
    "\n",
    "## so we conclude that rnn is better in all the above things than fnn by seeing the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c22756",
   "metadata": {},
   "source": [
    "# RNN (5B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75eda9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#using relu so that model is non linear and learns complex relations\n",
    "#used categorical_crossentropy for faster convergence and high penalty for mistakes\n",
    "#used adams for adaptive learning rate, faster concergence and as it requires less memory\n",
    "\n",
    "model_rnn = Sequential()\n",
    "model_rnn.add(Embedding(input_dim=len(model.key_to_index), output_dim=300, weights=[model.vectors], input_length=20, trainable=False))\n",
    "#model_rnn.add(Embedding(input_dim=len(model.vocab), output_dim=300, weights=[model.vectors], input_length=20, trainable=False))\n",
    "\n",
    "model_rnn.add(GRU(units=20,activation='relu'))\n",
    "model_rnn.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model_rnn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "checkpoint = ModelCheckpoint('best_model_1.h5', monitor='val_accuracy', save_best_only=True)\n",
    "\n",
    "# X_trainrnn_2 = np.array(X_train_2)\n",
    "# X_testrnn_2 = np.array(X_test_2)\n",
    "# y_trainrnn_2 = np.array(y_train_2)\n",
    "# y_testrnn_2 = np.array(y_test_2)\n",
    "\n",
    "# y_trainrnn_2 = to_categorical(y_trainrnn_2-1,num_classes = 3)\n",
    "# y_testrnn_2 = to_categorical(y_testrnn_2-1,num_classes = 3)\n",
    "\n",
    "\n",
    "# Fit the model\n",
    "model_rnn.fit(X_trainrnn_2, y_trainrnn_2, batch_size=32, epochs=15, validation_data=(X_testrnn_2, y_testrnn_2),callbacks=[checkpoint])\n",
    "best_model = load_model('best_model_1.h5')\n",
    "\n",
    "\n",
    "# Evaluate the model on the testing set\n",
    "loss, accuracy = best_model.evaluate(X_testrnn_2, y_testrnn_2)\n",
    "print('Test loss for rnn in 5b:', loss)\n",
    "print('Test accuracy for rnn in 5b:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91693aa7",
   "metadata": {},
   "source": [
    "# RNN(5c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d59e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using relu so that model is non linear and learns complex relations\n",
    "#used categorical_crossentropy for faster convergence and high penalty for mistakes\n",
    "#used adams for adaptive learning rate, faster concergence and as it requires less memory\n",
    "\n",
    "model_rnn = Sequential()\n",
    "model_rnn.add(Embedding(input_dim=len(model.key_to_index), output_dim=300, weights=[model.vectors], input_length=20, trainable=False))\n",
    "#model_rnn.add(Embedding(input_dim=len(model.vocab), output_dim=300, weights=[model.vectors], input_length=20, trainable=False))\n",
    "\n",
    "model_rnn.add(LSTM(units=20,activation='relu'))\n",
    "model_rnn.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model_rnn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "checkpoint = ModelCheckpoint('best_model_2.h5', monitor='val_accuracy', save_best_only=True)\n",
    "\n",
    "# X_trainrnn_2 = np.array(X_train_2)\n",
    "# X_testrnn_2 = np.array(X_test_2)\n",
    "# y_trainrnn_2 = np.array(y_train_2)\n",
    "# y_testrnn_2 = np.array(y_test_2)\n",
    "\n",
    "# y_trainrnn_2 = to_categorical(y_trainrnn_2-1,num_classes = 3)\n",
    "# y_testrnn_2 = to_categorical(y_testrnn_2-1,num_classes = 3)\n",
    "\n",
    "\n",
    "# Fit the model with hyper\n",
    "model_rnn.fit(X_trainrnn_2, y_trainrnn_2, batch_size=32, epochs=10, validation_data=(X_testrnn_2, y_testrnn_2),callbacks=[checkpoint])\n",
    "best_model = load_model('best_model_2.h5')\n",
    "\n",
    "\n",
    "# Evaluate the model on the testing set\n",
    "loss, accuracy = best_model.evaluate(X_testrnn_2, y_testrnn_2)\n",
    "print('Test loss for rnn in 5c:', loss)\n",
    "print('Test accuracyfor rnn in 5c:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42cd026",
   "metadata": {},
   "outputs": [],
   "source": [
    "##What do you conclude by comparing accuracy values you obtain by GRU,\n",
    "#LSTM, and simple RNN.\n",
    "\n",
    "\n",
    "###Observation: the best performing rnn out of all three is LSTM, then GRU and then simple rnn\n",
    "\n",
    "##inferences/justification: \n",
    "## maybe simple is suffering from vanishing gradients\n",
    "## LSTM due to its better and complex architecture is capturing context in words better. also it seems it is robust towards noise\n",
    "## gru seems to  be better in generalizing over data that is not seen before than simple rnn\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
